{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Yarışma submitlerini 1 ve 0 olarak yapılması yazısına göre yaparak düşük roc-auc score elde ettim. Probablity lerini late submission yaparak denediğimde skorumun yüksek olduğunu gördüm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# olasılıkları yüklediğimde aslında yaptıklarımın dogru oldugunu gördüm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgyukle.com/2021/02/21/L2bfKx.png\"/>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/turkiye-is-bankas-machine-learning-challenge-3/sample_submission.csv\n/kaggle/input/turkiye-is-bankas-machine-learning-challenge-3/monthly_expenditures.csv\n/kaggle/input/turkiye-is-bankas-machine-learning-challenge-3/train.csv\n/kaggle/input/turkiye-is-bankas-machine-learning-challenge-3/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom scipy.stats import norm\n\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\nfrom collections import Counter\nimport scipy.stats as stats\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/turkiye-is-bankas-machine-learning-challenge-3/'\n\ntrain_df = pd.read_csv(path+'train.csv', parse_dates=['tarih'])\ntest_df = pd.read_csv(path+'test.csv', parse_dates=['tarih'])\nmonthly_exp = pd.read_csv(path+'monthly_expenditures.csv', parse_dates=['tarih'])\nsub = pd.read_csv(path+'sample_submission.csv')\n\nfor df in [train_df, test_df]:\n    df.drop(['tarih','meslek_grubu'], axis=1, inplace=True)\n    \ny = train_df['target']\ntrain_df.drop('target', axis=1, inplace=True)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Incorrect values & Some changes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tarihlerin farkı ay olduğu için ay olarak çeviriyorum\nmonthly_exp['ay'] = [x.month for x in monthly_exp['tarih']]\n\nmonthly_exp.drop('tarih', axis=1, inplace=True)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tl to dolar\n# o aykı dolar kuruyla tl -> dolara çeviriyorum\nmonthly_exp.loc[monthly_exp['ay'] == 1, 'aylik_toplam_tutar'] = monthly_exp[monthly_exp['ay'] == 1]['aylik_toplam_tutar'] / 5.3759\nmonthly_exp.loc[monthly_exp['ay'] == 2, 'aylik_toplam_tutar'] = monthly_exp[monthly_exp['ay'] == 2]['aylik_toplam_tutar'] / 5.2769\nmonthly_exp.loc[monthly_exp['ay'] == 3, 'aylik_toplam_tutar'] = monthly_exp[monthly_exp['ay'] == 3]['aylik_toplam_tutar'] / 5.4666\nmonthly_exp.loc[monthly_exp['ay'] == 4, 'aylik_toplam_tutar'] = monthly_exp[monthly_exp['ay'] == 4]['aylik_toplam_tutar'] / 5.7617\nmonthly_exp.loc[monthly_exp['ay'] == 5, 'aylik_toplam_tutar'] = monthly_exp[monthly_exp['ay'] == 5]['aylik_toplam_tutar'] / 6.0560\nmonthly_exp.loc[monthly_exp['ay'] == 6, 'aylik_toplam_tutar'] = monthly_exp[monthly_exp['ay'] == 6]['aylik_toplam_tutar'] / 5.8127","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# islem_adedi: less than 0\nabs_islem_adedi = monthly_exp[monthly_exp['islem_adedi'] < 0]['islem_adedi'].apply(lambda x: abs(x))\nmonthly_exp.loc[monthly_exp['islem_adedi'] < 0, 'islem_adedi'] = abs_islem_adedi","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# week to year\ntrain_df['kidem_suresi'] = train_df['kidem_suresi'].apply(lambda x: (x/4.34812141)/12)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# week to year\ntest_df['kidem_suresi'] = test_df['kidem_suresi'].apply(lambda x: (x/4.34812141)/12)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill the kidem_suresi < 0 by mean of the similars\nfor index in list(train_df[train_df['kidem_suresi'] < 0].index):\n    similars_median = np.median(train_df[(train_df['kidem_suresi'] > 0) & (train_df['yas'] == train_df.iloc[index]['yas']) & \n         (train_df['egitim'] == train_df.iloc[index]['egitim']) & (train_df['is_durumu'] == train_df.iloc[index]['is_durumu'])]['kidem_suresi'], axis=0)\n    \n    train_df.iloc[index, 2] = similars_median","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill the kidem_suresi < 0 by mean of the similars\nfor index in list(test_df[test_df['kidem_suresi'] < 0].index):\n    similars_median = np.median(train_df[(train_df['kidem_suresi'] > 0) & (train_df['yas'] == train_df.iloc[index]['yas']) & \n         (train_df['egitim'] == train_df.iloc[index]['egitim']) & (train_df['is_durumu'] == train_df.iloc[index]['is_durumu'])]['kidem_suresi'], axis=0)\n    \n    test_df.iloc[index, 2] = similars_median","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing values, mode ile dolduruyorum\nfor df in [train_df, test_df]:  \n    df['egitim'].fillna(train_df['egitim'].value_counts().index[0], inplace=True)\n    df['is_durumu'].fillna(train_df['is_durumu'].value_counts().index[0], inplace=True)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Log "},{"metadata":{"trusted":true},"cell_type":"code","source":"# skewness engellemek için log uyguluyorum\ntrain_log = train_df[['yas']].apply(np.log, axis = 1)\ntest_log = test_df[['yas']].apply(np.log, axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train_log)\n\ntrain_df['yas']= scaler.transform(train_log)\ntest_df['yas']= scaler.transform(test_log)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# log tanımsız/sonsuz olmaması için kontrol\ndef neg_to_zero(x):\n    if x <= 0:\n        return 1\n    else:\n        return x","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['kidem_suresi'] = [neg_to_zero(x) for x in train_df.kidem_suresi]\ntest_df['kidem_suresi'] = [neg_to_zero(x) for x in test_df.kidem_suresi]\n\ntrain_log = train_df[['kidem_suresi']].apply(np.log, axis = 1)\ntest_log = test_df[['kidem_suresi']].apply(np.log, axis = 1)\n\nscaler = StandardScaler()\nscaler.fit(train_log)\n\ntrain_df['kidem_suresi']= scaler.transform(train_log)\ntest_df['kidem_suresi']= scaler.transform(test_log)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mont_log = monthly_exp[['islem_adedi']].apply(np.log, axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(mont_log)\n\nmonthly_exp['islem_adedi'] = scaler.transform(mont_log)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_exp['aylik_toplam_tutar'] = [neg_to_zero(x) for x in monthly_exp.aylik_toplam_tutar]\n\nmont_log = monthly_exp[['aylik_toplam_tutar']].apply(np.log, axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(mont_log)\n\nmonthly_exp['aylik_toplam_tutar'] = scaler.transform(mont_log)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Join Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n\ndel train_df\ndel test_df","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deneme_df = pd.DataFrame()\n\ndeneme_df['musteri'] = monthly_exp.musteri.unique()\n\n# for i in range(1,7):\n#     groupby_ay = monthly_exp[monthly_exp['ay'] == i].groupby('musteri')['islem_adedi','aylik_toplam_tutar'].sum()\n#     tutar_islem = groupby_ay['aylik_toplam_tutar'] / groupby_ay['islem_adedi']\n#     deneme_df = deneme_df.merge(tutar_islem.rename('ay_'+str(i)), on='musteri', how='left')\n    \n# deneme_df.fillna(0, inplace=True)\n# deneme_df['ortalama_islem_harcama'] = np.mean(deneme_df.loc[:,'ay_1':'ay_6'], axis=1)\n\n# deneme_df.drop(deneme_df.loc[:,'ay_1':'ay_6'].columns, axis=1, inplace=True)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ay ve sektöre göre harcamaları column'a dönüştürüyorum\nfor month in range(1,7):\n    for feature in list(monthly_exp['sektor'].value_counts().index):\n        groupby_ay = monthly_exp[monthly_exp['ay'] == month].groupby(['musteri','sektor'])['islem_adedi','aylik_toplam_tutar'].sum()\n        sektor = groupby_ay.xs(feature, level=1, drop_level=False)\n        take_sektor_islem_tutar = sektor['aylik_toplam_tutar'] #/ sektor['islem_adedi']\n        deneme_df = deneme_df.merge(take_sektor_islem_tutar.rename('ay_'+str(month)+'_'+feature), on='musteri', how='left')\n        \ndeneme_df.fillna(0, inplace=True)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yaptıgı harcamaya göre bir müşteri type olusturuyorum\n\n# islem_toplam = monthly_exp.groupby(['musteri'])['islem_adedi','aylik_toplam_tutar'].sum()['islem_adedi']\ntutar_toplam = monthly_exp.groupby(['musteri'])['islem_adedi','aylik_toplam_tutar'].sum()['aylik_toplam_tutar']\n#deneme_df['per_type'] = pd.qcut(tutar_toplam / islem_toplam, 4).values\ndeneme_df['harcama_type'] = pd.qcut(tutar_toplam, 6).values","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aylik islem adetleri ile yeni columnlar yaratıyorum\nfor month in range(1,7):\n    groupby_ay = monthly_exp[monthly_exp['ay'] == month].groupby(['musteri'])['islem_adedi'].sum()\n    deneme_df = deneme_df.merge(groupby_ay.rename('ay_'+str(month)), on='musteri', how='left')\n    \ndeneme_df.loc[:,'ay_1':'ay_6'] = deneme_df.loc[:,'ay_1':'ay_6'].fillna(0)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# yaptıgı isleme göre yeni müsteri type olustuyorum\ndeneme_df['islem_type'] = deneme_df.loc[:,'ay_1':'ay_6'].sum(axis=1)\n\ndeneme_df['islem_type'] = pd.qcut(deneme_df['islem_type'], 6).values","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = full_df.merge(deneme_df, on='musteri', suffixes=('_left', '_right'), how='inner')","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# son hali\nfull_df.head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"      musteri       yas  kidem_suresi      egitim   is_durumu  \\\n0  23b2476c8a  2.110481     -0.095449  5f8ca0f83b  915be3a7a4   \n1  5a7e3a7122  1.607754      1.244046  01a4f2c96c  915be3a7a4   \n2  2ec95c7499  1.499499      1.184619  7e6640bfe0  d36d84f51a   \n3  54399dac33  1.035956      0.636536  5f8ca0f83b  991c4998fb   \n4  97a74b2e58  1.816177      0.834957  7bb291e291  242927d0f5   \n\n   ay_1_RESTORAN_CATER  ay_1_GIYIM_AKSESUAR  ay_1_BENZIN_YAKIT  \\\n0            -0.338302            -0.058719           0.000000   \n1            -1.186297             0.000000           0.000000   \n2             0.000000             0.000000           0.000000   \n3             0.000000             0.000000           0.000000   \n4             0.000000             0.000000           0.336609   \n\n   ay_1_ELKT_ESYA_BILG  ay_1_TASIMACILIK  ...  ay_6_KUYUMCU  \\\n0             0.000000               0.0  ...           0.0   \n1            -0.526230               0.0  ...           0.0   \n2             0.000000               0.0  ...           0.0   \n3             0.000000               0.0  ...           0.0   \n4             0.807552               0.0  ...           0.0   \n\n   ay_6_KLP_DERNK_SOSY      harcama_type      ay_1      ay_2      ay_3  \\\n0                  0.0  (-3.841, -1.946] -1.593089  0.876684  0.068691   \n1                  0.0    (-0.84, 0.322] -1.615986 -0.807993 -0.807993   \n2                  0.0   (-1.946, -0.84]  0.000000 -0.807993  0.000000   \n3                  0.0   (-1.946, -0.84]  0.000000  0.000000  0.000000   \n4                  0.0    (0.322, 3.248] -0.762199  0.000000  0.000000   \n\n       ay_4      ay_5      ay_6                     islem_type  \n0 -0.276160  0.508936  0.000000               (-0.808, 0.0458]  \n1 -1.615986 -1.593089  0.853787  (-19.563000000000002, -3.371]  \n2  0.000000  0.000000  0.000000               (-1.832, -0.808]  \n3  0.000000 -0.807993  0.000000               (-1.832, -0.808]  \n4  0.000000  0.000000  0.000000               (-0.808, 0.0458]  \n\n[5 rows x 91 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>musteri</th>\n      <th>yas</th>\n      <th>kidem_suresi</th>\n      <th>egitim</th>\n      <th>is_durumu</th>\n      <th>ay_1_RESTORAN_CATER</th>\n      <th>ay_1_GIYIM_AKSESUAR</th>\n      <th>ay_1_BENZIN_YAKIT</th>\n      <th>ay_1_ELKT_ESYA_BILG</th>\n      <th>ay_1_TASIMACILIK</th>\n      <th>...</th>\n      <th>ay_6_KUYUMCU</th>\n      <th>ay_6_KLP_DERNK_SOSY</th>\n      <th>harcama_type</th>\n      <th>ay_1</th>\n      <th>ay_2</th>\n      <th>ay_3</th>\n      <th>ay_4</th>\n      <th>ay_5</th>\n      <th>ay_6</th>\n      <th>islem_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23b2476c8a</td>\n      <td>2.110481</td>\n      <td>-0.095449</td>\n      <td>5f8ca0f83b</td>\n      <td>915be3a7a4</td>\n      <td>-0.338302</td>\n      <td>-0.058719</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(-3.841, -1.946]</td>\n      <td>-1.593089</td>\n      <td>0.876684</td>\n      <td>0.068691</td>\n      <td>-0.276160</td>\n      <td>0.508936</td>\n      <td>0.000000</td>\n      <td>(-0.808, 0.0458]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5a7e3a7122</td>\n      <td>1.607754</td>\n      <td>1.244046</td>\n      <td>01a4f2c96c</td>\n      <td>915be3a7a4</td>\n      <td>-1.186297</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.526230</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(-0.84, 0.322]</td>\n      <td>-1.615986</td>\n      <td>-0.807993</td>\n      <td>-0.807993</td>\n      <td>-1.615986</td>\n      <td>-1.593089</td>\n      <td>0.853787</td>\n      <td>(-19.563000000000002, -3.371]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2ec95c7499</td>\n      <td>1.499499</td>\n      <td>1.184619</td>\n      <td>7e6640bfe0</td>\n      <td>d36d84f51a</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(-1.946, -0.84]</td>\n      <td>0.000000</td>\n      <td>-0.807993</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(-1.832, -0.808]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>54399dac33</td>\n      <td>1.035956</td>\n      <td>0.636536</td>\n      <td>5f8ca0f83b</td>\n      <td>991c4998fb</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(-1.946, -0.84]</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.807993</td>\n      <td>0.000000</td>\n      <td>(-1.832, -0.808]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>97a74b2e58</td>\n      <td>1.816177</td>\n      <td>0.834957</td>\n      <td>7bb291e291</td>\n      <td>242927d0f5</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.336609</td>\n      <td>0.807552</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.322, 3.248]</td>\n      <td>-0.762199</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(-0.808, 0.0458]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 91 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kidem_suresi içinde -1 değerler var dikkat edilmeli\n#full_df[full_df['kidem_suresi'] == -1]['musteri'].isin(idx)\n\n# 4 , 7","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_df.drop('musteri', axis=1, inplace=True)","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create model df"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_df = pd.DataFrame()","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_df['yas'] = pd.qcut(full_df['yas'], 6)\n\nmodel_df['kidem_suresi'] = pd.qcut(full_df['kidem_suresi'], 6)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sadece bir tane bu eğitimden var onu mode ile değiştir\n# fazla column olusturmanın gereksiz olcagını düşündüm\nfull_df.loc[48736, 'egitim'] = '7e6640bfe0'","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13 grup var","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grafikler çizerek benzer evlilik oranlarına sahip is durumlarını ortak noktalarda topladım\nmodel_df['is_durumu'] = full_df['is_durumu'].map({'3773727d6e':0, '83a26fc2fd':0, '51be29729b':0, 'd36d84f51a':0,\n                                                  'eb35a5eb6b':1, '289777e76d':1, 'b026b8ee68':0,\n                                                  '915be3a7a4':2, '991c4998fb':2,\n                                                  'a996720382':3, 'f1fcd26d00':3,\n                                                  '242927d0f5':4, 'ba7b390fc4':4})","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode\ndummied_is_durumu = pd.get_dummies(model_df['is_durumu'], prefix='is_durumu')\n\nmodel_df = pd.concat([model_df, dummied_is_durumu], axis=1)\nmodel_df.drop('is_durumu', axis=1, inplace=True)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encode\nfor column in ['yas', 'kidem_suresi']:\n    le = LabelEncoder()\n    model_df[column] = le.fit_transform(model_df[column])","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode\ndummied_egitim = pd.get_dummies(full_df['egitim'], prefix='egitim')\nmodel_df = pd.concat([model_df, dummied_egitim], axis=1)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# le3 = LabelEncoder()\n# le4 = LabelEncoder()\n# le5 = LabelEncoder()\n\n# model_df['islem_adedi_type'] = le3.fit_transform(full_df['islem_adedi_type'])\n# model_df['harcama_type'] = le4.fit_transform(full_df['harcama_type'])\n# model_df['per_type'] = le5.fit_transform(full_df['per_type'])","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_df['yas'] = full_df['yas']\n# model_df['kidem_suresi'] = full_df['kidem_suresi']","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dummied_meslek_grubu = pd.get_dummies(full_df['meslek_grubu'], prefix='meslek_grubu')\n# model_df = pd.concat([model_df, dummied_meslek_grubu], axis=1)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_df = pd.concat([model_df, full_df.iloc[:,5:]], axis=1)\n\ndel full_df\ndel monthly_exp","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in ['harcama_type', 'islem_type']:\n    le = LabelEncoder()\n    model_df[column] = le.fit_transform(model_df[column])","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for column in list(model_df.loc[:,'ay_1_RESTORAN_CATER':'ay_6_KLP_DERNK_SOSY'].columns):\n#     model_df[column] = pd.cut(model_df[column], 6)\n    \n# for column in list(model_df.loc[:,'ay_1':'ay_6'].columns):\n#     model_df[column] = pd.cut(model_df[column], 6)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for column in list(model_df.loc[:,'ay_1_RESTORAN_CATER':'ay_6_KLP_DERNK_SOSY'].columns):\n#     le = LabelEncoder()\n#     model_df[column] = le.fit_transform(model_df[column])\n    \n# for column in list(model_df.loc[:,'ay_1':'ay_6'].columns):\n#     le = LabelEncoder()\n#     model_df[column] = le.fit_transform(model_df[column])","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modele sokulcak verinin son hali\nmodel_df.head()","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"   yas  kidem_suresi  is_durumu_0  is_durumu_1  is_durumu_2  is_durumu_3  \\\n0    5             2            0            0            1            0   \n1    5             5            0            0            1            0   \n2    5             5            1            0            0            0   \n3    4             4            0            0            1            0   \n4    5             4            0            0            0            0   \n\n   is_durumu_4  egitim_01a4f2c96c  egitim_5f8ca0f83b  egitim_7bb291e291  ...  \\\n0            0                  0                  1                  0  ...   \n1            0                  1                  0                  0  ...   \n2            0                  0                  0                  0  ...   \n3            0                  0                  1                  0  ...   \n4            1                  0                  0                  1  ...   \n\n   ay_6_KUYUMCU  ay_6_KLP_DERNK_SOSY  harcama_type      ay_1      ay_2  \\\n0           0.0                  0.0             1 -1.593089  0.876684   \n1           0.0                  0.0             3 -1.615986 -0.807993   \n2           0.0                  0.0             2  0.000000 -0.807993   \n3           0.0                  0.0             2  0.000000  0.000000   \n4           0.0                  0.0             4 -0.762199  0.000000   \n\n       ay_3      ay_4      ay_5      ay_6  islem_type  \n0  0.068691 -0.276160  0.508936  0.000000           3  \n1 -0.807993 -1.615986 -1.593089  0.853787           0  \n2  0.000000  0.000000  0.000000  0.000000           2  \n3  0.000000  0.000000 -0.807993  0.000000           2  \n4  0.000000  0.000000  0.000000  0.000000           3  \n\n[5 rows x 97 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yas</th>\n      <th>kidem_suresi</th>\n      <th>is_durumu_0</th>\n      <th>is_durumu_1</th>\n      <th>is_durumu_2</th>\n      <th>is_durumu_3</th>\n      <th>is_durumu_4</th>\n      <th>egitim_01a4f2c96c</th>\n      <th>egitim_5f8ca0f83b</th>\n      <th>egitim_7bb291e291</th>\n      <th>...</th>\n      <th>ay_6_KUYUMCU</th>\n      <th>ay_6_KLP_DERNK_SOSY</th>\n      <th>harcama_type</th>\n      <th>ay_1</th>\n      <th>ay_2</th>\n      <th>ay_3</th>\n      <th>ay_4</th>\n      <th>ay_5</th>\n      <th>ay_6</th>\n      <th>islem_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>-1.593089</td>\n      <td>0.876684</td>\n      <td>0.068691</td>\n      <td>-0.276160</td>\n      <td>0.508936</td>\n      <td>0.000000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>-1.615986</td>\n      <td>-0.807993</td>\n      <td>-0.807993</td>\n      <td>-1.615986</td>\n      <td>-1.593089</td>\n      <td>0.853787</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>-0.807993</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.807993</td>\n      <td>0.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>-0.762199</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 97 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split train ve test\nX_train = model_df[:60000]\nX_test = model_df[60000:]","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test = X_test.reset_index(drop=True)\n# X_train = X_train.reset_index(drop=True)\n\n# X_train = X_train.sample(frac=1, random_state=1461)\n# y = y.sample(frac=1, random_state=1461)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_df\n# del X_train\n# del X_test","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sampler"},{"metadata":{"trusted":true},"cell_type":"code","source":"# imbalance problem oldugu için oversampling & undersampling & kombinasyonlarını denemek istedim\n# problemler şöyle oldu\n# - undersampling yapınca çok az veri ile eğitim gerçekleşiyor yetersiz kalıyor\n# - oversampling overfitting olma olasılıgı yüksek. Verinin dikkatli kullanılması lazım\n# - kombinasyonlarında da overfitting sorunu var, cross validation aldanmamak gerekiyor\n\n# XGBoost, RandomForest, Keras da weight vererek, normal veri üzerinde de denerim. Weight ile daha stabil ve biraz daha iyi sonuçlar elde edilebiliyor","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import NearMiss, EditedNearestNeighbours\nfrom sklearn.utils import shuffle\nfrom imblearn.under_sampling import TomekLinks","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kendimce undersampling yaptım\ndeneme_x_1 = X_train.iloc[y[y==1].index]\ndeneme_y_1 = y[y==1]\n\nsample = y[y==0].sample(2440)\ndeneme_x_0 = X_train.iloc[sample.index]\ndeneme_y_0 = sample\n\ndeneme_X = pd.concat([deneme_x_1, deneme_x_0], axis=0) \ndeneme_y = pd.concat([deneme_y_1, deneme_y_0], axis=0).astype('int32')\n#np.concatenate\n\ndeneme_y = deneme_y.sample(frac=1, random_state=1461)\ndeneme_X = deneme_X.sample(frac=1, random_state=1461)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imblearn kütüphanesi kullanarak undersampling, oversampling ve kombinasyonlarını yaptım\nx_train, x_val, y_train, y_val = train_test_split(X_train, y,\n                                                  test_size = .33,\n                                                  random_state=14,\n                                                 stratify=y)\n\n# oversamplers\novs = RandomOverSampler(random_state=14)\nx_res, y_res = ovs.fit_sample(x_train, y_train)\n\nsmk = SMOTETomek(random_state=14)\nx_res2, y_res2 = smk.fit_sample(x_train, y_train)\n\n# undersamplers\nous = RandomUnderSampler(random_state=14)\nx_res3,y_res3 = ous.fit_sample(X_train, y)\n\nnm = NearMiss()\nx_res4,y_res4 = nm.fit_sample(X_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kombinasyon\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.8)\n\nx_overed, y_overed = over.fit_sample(x_train, y_train)\nx_res5, y_res5 = under.fit_sample(x_overed, y_overed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kombinasyon\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n\nx_res6, y_res6 = resample.fit_sample(x_train, y_train)\ny_res6 = y_res6.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kombinasyon\nresample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n\nx_res7, y_res7 = resample.fit_sample(x_train, y_train)\ny_res7 = y_res7.astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, accuracy_score, f1_score, recall_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve, cross_validate, RepeatedStratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pred(model, X, y, X_test):\n    model.fit(X, y)\n    preds = model.predict(X_test)\n    return preds","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit_csv(model, X, y, X_test, name='submit.csv'):\n    \n    model.fit(X, y)\n    \n    preds = model.predict(X_test)\n    sub['target'] = preds\n    print(Counter(preds))\n    sub.to_csv(name, index=False)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def estimate(models, X, y):\n    \n    X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y, test_size=.25, random_state=14)\n    \n    for model in models:\n\n        model.fit(X_train_s, y_train_s)\n\n        y_scores = model.predict_proba(X_test_s)\n        y_scores = y_scores[:,1]\n\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_s, y_scores)\n\n        \n        print('-----',model.__class__.__name__,'-----')\n        auroc = roc_auc_score(y_test_s, y_scores)\n        print(classification_report(y_test_s, model.predict(X_test_s)))\n        print(\"ROC-AUC Score:\", auroc)\n        print(\"Confusion Matrix:\\n\",confusion_matrix(y_test_s,model.predict(X_test_s)),'\\n')\n\n        if len(models) == 1: \n            plt.figure(figsize=(14, 7))\n            plot_roc_curve(false_positive_rate, true_positive_rate)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# daha karmaşık modeller denemeden önce basit ve farklı modeller üzerinde hızlı bir şekilde iyi skor alablir miyim onu denedim\n\nkf = StratifiedKFold(10, shuffle=True, random_state=14)\n\nm_logreg = LogisticRegression(random_state=14)\n\nm_dtc = DecisionTreeClassifier(random_state=14)\n\nm_ada = AdaBoostClassifier(DecisionTreeClassifier(random_state=14),random_state=14,learning_rate=0.1)\n\nm_rf = RandomForestClassifier(random_state=14)\n\nm_ext = ExtraTreesClassifier(random_state=14)\n\nm_gdb = GradientBoostingClassifier(random_state=14)\n\nm_mlp = MLPClassifier(random_state=14)\n\nm_knn = KNeighborsClassifier()\n\nm_lda = LinearDiscriminantAnalysis()\n\n# models = [m_dtc, m_ada, m_dtc, m_rf, m_ext, m_gdb, m_knn, m_lda]\nmodels = [m_logreg, m_dtc, m_ada, m_rf, m_ext, m_gdb, m_mlp, m_knn, m_lda]","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Models\n\ncv = StratifiedKFold(10, shuffle=True, random_state=42)\n\nrf = RandomForestClassifier(criterion='gini',\n                            n_estimators=1750,\n                            max_depth=7,\n                            min_samples_split=6,\n                            min_samples_leaf=6,\n                            max_features='auto',\n                            oob_score=True,\n                            random_state=42,\n                            n_jobs=-1,\n                            verbose=0)\n\nlg = lgb.LGBMClassifier(max_bin=4,\n                        num_iterations=550,\n                        learning_rate=0.0114,\n                        max_depth=3,\n                        num_leaves=7,\n                        colsample_bytree=0.35,\n                        random_state=42,\n                        n_jobs=-1)\n\nxg = xgb.XGBClassifier(\n    n_estimators=2800,\n    min_child_weight=0.1,\n    learning_rate=0.002,\n    max_depth=2,\n    subsample=0.47,\n    colsample_bytree=0.35,\n    gamma=0.4,\n    reg_lambda=0.4,\n    random_state=42,\n    n_jobs=-1,\n)\n\nlogreg = LogisticRegression(n_jobs=-1, solver='newton-cg')\n\ngb = GradientBoostingClassifier(random_state=42)\n\ngnb = GaussianNB()\n\nmlp = MLPClassifier(random_state=42)\n\nestimators = [rf, lg, xg, gb, logreg, gnb, mlp]","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_check(X, y, estimators, cv):\n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        #    model_table.loc[row_index, 'MLA Parameters'] = str(est.get_params())\n\n        cv_results = cross_validate(\n            est,\n            X,\n            y,\n            cv=cv,\n            scoring='accuracy',\n            return_train_score=True,\n            n_jobs=-1\n        )\n\n        model_table.loc[row_index, 'Train Accuracy Mean'] = cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test Accuracy Mean'] = cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test Accuracy Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_models = model_check(deneme_X, deneme_y, models, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_models = model_check(deneme_X, deneme_y, estimators, cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_models = model_check(deneme_X, deneme_y, [RFC_best], cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GradientBoosting iyi bir sonuç vermişti bu yüzden parametre tunning yaparak sonuçlarını kontrol ettim\npar = {'n_estimators': 100,\n   'min_samples_leaf': 125,\n   'max_features': 0.5,\n   'max_depth': 4,\n   'loss': 'exponential',\n   'learning_rate': 0.1}\ngb = GradientBoostingClassifier(random_state=42)\ngb.set_params(**par)\nraw_models = model_check(deneme_X, deneme_y, [gb], cv)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RFC Parameters tunning \nRFC = GradientBoostingClassifier(random_state=14)\nkf2 = StratifiedKFold(5, shuffle=True, random_state=14)\n\nparams = {'loss' : [\"deviance\",'exponential'],\n              'n_estimators' : [100,200,400,600],\n              'learning_rate': [0.1,0.3, 0.05, 0.01,1],\n              'max_depth': [None,2,3,4],\n              'min_samples_leaf': [15,25,50,100,150],\n              'max_features': [0.7,0.5,0.2,0.3] \n              }\n\ngsRFC = RandomizedSearchCV(RFC, param_distributions = params,n_iter=5,  cv=kf2, scoring=\"roc_auc\", n_jobs= -1, verbose = 1)\n\ngsRFC.fit(deneme_X, deneme_y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","execution_count":62,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'deneme_X' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-5c39f62ff2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgsRFC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRFC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"roc_auc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgsRFC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeneme_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeneme_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mRFC_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgsRFC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'deneme_X' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC = RandomForestClassifier(random_state=14)\n\nRFC.set_params(**{'bootstrap': False,\n 'criterion': 'entropy',\n 'max_depth': None,\n 'max_features': 2,\n 'min_samples_leaf': 3,\n 'min_samples_split': 2,\n 'n_estimators': 300})\n\nETC = ExtraTreesClassifier(random_state=14)\n\nETC.set_params(**{'bootstrap': False,\n 'criterion': 'gini',\n 'max_depth': 5,\n 'max_features': 1,\n 'min_samples_leaf': 3,\n 'min_samples_split': 2,\n 'n_estimators': 500})\n\nLDA = LinearDiscriminantAnalysis()\n\nKNN = KNeighborsClassifier()\n\nKNN.set_params(**{'algorithm': 'auto',\n 'leaf_size': 1,\n 'n_neighbors': 10,\n 'p': 1,\n 'weights': 'uniform'})\n\nGBC = GradientBoostingClassifier(random_state=14)\nGBC.set_params(**{'learning_rate': 0.05,\n 'loss': 'deviance',\n 'max_depth': 4,\n 'max_features': 0.1,\n 'min_samples_leaf': 50,\n 'n_estimators': 200})\n\nprint('OK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_t, x_v, y_t, y_v = train_test_split(deneme_X, deneme_y,\n                                                  test_size = .33,\n                                                  random_state=14,\n                                                 stratify=deneme_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = {'updater': ['grow_gpu_hist'],\n 'tree_method': ['gpu_hist'],\n 'subsample': [1.0],\n 'predictor': ['gpu_predictor'],\n 'n_estimators': [600],\n 'min_child_weight': [10],\n 'max_depth': [8],\n 'learning_rate': [0.05],\n 'gamma': [0.5],\n 'colsample_bytree': [0.6]}\n\n\nparams = {\n        'min_child_weight': [2,3,4,5, 10, 15],\n        'gamma': [0.5, 1, 1.5, 2, 3.5, 5],\n        'subsample': [0.4, 0.6, 0.8, 1.0],\n        'colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 6, 8],\n        'n_estimators': [400,600,800,1000,1200,1600,2000,2500],\n        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.5, 1, 1.5]\n        }\n\nparams['tree_method'] = ['gpu_hist']\nparams['updater'] = ['grow_gpu_hist']\nparams['predictor'] = ['gpu_predictor']\n\nclass_weight = int(y.value_counts()[0] / y.value_counts()[1])\n# class_weight = 1\nxgb = XGBClassifier(objective='binary:logistic',\n                    silent=True, nthread=1,eval_metric='auc',verbosity=0,scale_post_weight=class_weight)\n\nfolds = 3\nparam_comb = 15\nSEED = 1461\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = SEED)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=-1, cv=skf.split(X_train_scaled, y), verbose=3, random_state=SEED)\n\nrandom_search.fit(X_train_scaled, y)\nprint('OK')","execution_count":63,"outputs":[{"output_type":"stream","text":"Fitting 3 folds for each of 15 candidates, totalling 45 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  2.9min finished\n","name":"stderr"},{"output_type":"stream","text":"OK\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(random_search.cv_results_['split0_test_score'])\nprint(random_search.cv_results_['split1_test_score'])\nprint(random_search.cv_results_['split2_test_score'])\nprint(random_search.cv_results_['mean_test_score'])\nprint(random_search.cv_results_['std_test_score'])\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)","execution_count":64,"outputs":[{"output_type":"stream","text":"[0.76072119 0.73347694 0.76660531 0.76573295 0.64852233 0.75100277\n 0.62057194 0.64723043 0.76954158 0.76938154 0.64703952 0.76684228\n 0.73768403 0.66562272 0.7605624 ]\n[0.75762712 0.72641666 0.76323709 0.76327151 0.63424853 0.74957182\n 0.6239143  0.62852789 0.76568006 0.76451992 0.62888833 0.76353986\n 0.74421027 0.65000961 0.75901038]\n[0.7601529  0.7358902  0.75895046 0.75566941 0.65431348 0.75478082\n 0.6183301  0.65545653 0.76112755 0.7617596  0.6595446  0.76184128\n 0.74358722 0.65015469 0.75416045]\n[0.75950041 0.73192793 0.76293095 0.76155796 0.64569478 0.75178514\n 0.62093878 0.64373828 0.76544973 0.76522036 0.64515748 0.76407447\n 0.74182717 0.65526234 0.75791108]\n[0.00134478 0.00401966 0.00313257 0.00428337 0.00843196 0.00219735\n 0.00229445 0.01126748 0.00343887 0.00315081 0.01258593 0.00207635\n 0.00294067 0.00732614 0.00272673]\n\n Best normalized gini score for 3-fold search with 15 parameter combinations:\n0.530899464351678\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = random_search.predict_proba(X_test_scaled)[:,1]\nsub.to_csv('deneme2.csv', index=False)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(confusion_matrix(y_v, random_search.predict(x_v)))\n# print(classification_report(y_v, random_search.predict(x_v)))\n\nmodel = random_search\nfinal_preds = model.predict_proba(X_train)[:,1]\nfpr, tpr, thresholds = roc_curve(y, final_preds)\n# thresholds\naccuracy_ls = []\n\nfor thres in thresholds:\n    y_pred = np.where(final_preds>thres,1,0)\n    accuracy_ls.append(roc_auc_score(y, y_pred))\n    \naccuracy_ls = pd.concat([pd.Series(thresholds), pd.Series(accuracy_ls)],\n                        axis=1)\naccuracy_ls.columns = ['thresholds', 'roc']\naccuracy_ls.sort_values(by='roc', ascending=False, inplace=True)\naccuracy_ls.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train_scaled, label=y)\ndtest = xgb.DMatrix(X_test_scaled)\n\nparamlar = {'updater': 'grow_gpu_hist',\n 'tree_method': 'gpu_hist',\n 'subsample': 1.0,\n 'predictor': 'gpu_predictor',\n 'n_estimators': 800,\n 'min_child_weight': 3,\n 'max_depth': 4,\n 'learning_rate': 0.1,\n 'gamma': 5,\n 'colsample_bytree': 0.8}\n\nparams['eval_metric'] = 'auc'\nparams['objective'] = 'binary:logistic'\nparams['scale_pos_weight'] = np.sqrt(int(y.value_counts()[0] / y.value_counts()[1]))\n\nbst = xgb.train(paramlar, dtrain, 35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_preds = bst.predict(dtrain)\nfpr, tpr, thresholds = roc_curve(y, final_preds)\n# thresholds\naccuracy_ls = []\nfor thres in thresholds:\n    y_pred = np.where(final_preds>thres,1,0)\n    accuracy_ls.append(roc_auc_score(y, y_pred))\n    \naccuracy_ls = pd.concat([pd.Series(thresholds), pd.Series(accuracy_ls)],\n                        axis=1)\naccuracy_ls.columns = ['thresholds', 'roc']\naccuracy_ls.sort_values(by='roc', ascending=False, inplace=True)\naccuracy_ls.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submision yaptıgım notebook şansız olaydan dolayı kaybettiğim için burada submit dosyam bulunmuyor\n# XGBoost'u kendi yaptıgım undersampling verisi ile eğittim ve hyperparameter tunning ile kendi skorumu elde ettim\n\n# proba yerine 0 ve 1 ler ile submit ettiğim için daha düşük aldıgımı düşünüyorum cv de 77-78 gibi auc-roc score elde ediliyor","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}